{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first DCGAN ðŸŒ±\n",
    "- DCGAN [paper](https://arxiv.org/abs/1511.06434)\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Gan Archtecture Scheme</b>\n",
    "</font>\n",
    "</summary>\n",
    "<div>\n",
    "<img src = \"layers.png\" width=800>\n",
    "</div>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, hidden_units) -> None:\n",
    "        \"\"\"Discriminator model for DCGAN architecture based\n",
    "         on Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks paper.\n",
    "         Paper: https://arxiv.org/abs/1511.06434\n",
    "\n",
    "        Args:\n",
    "            img_channels (int): Number of channels of the image (for example for RGB its 3).\n",
    "            hidden_units (int): Number of neurons in hidden layer.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # First discriminator block (bo batch norm)\n",
    "            nn.Conv2d(img_channels, hidden_units, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Rest of the blocks...\n",
    "            self._block(in_channels=hidden_units, out_channels=hidden_units * 2),\n",
    "            self._block(in_channels=hidden_units * 2, out_channels=hidden_units * 4),\n",
    "            self._block(in_channels=hidden_units * 4, out_channels=hidden_units * 8),\n",
    "            nn.Conv2d(in_channels=hidden_units * 8, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(), # Output range [0, 1]\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel__size=4, stride=2, padding=1) -> torch.Tensor:\n",
    "        \"\"\"Creates a single discriminator block consisting of a convolution layer, batch normalization and Leaky ReLU.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "                out_channels (int): Number of output channels.\n",
    "                kernel_size (int, optional): Size of convolution filter. Defaults to 4.\n",
    "                stride (int, optional): Stride size. Defaults to 2.\n",
    "                padding (int, optional): Amount of pixels added around the image. Defaults to 1.\n",
    "        Returns:\n",
    "            torch.Tensor: Returns a tensor with performed convolutions.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel__size, stride, padding, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim: int, img_channels: int, hidden_units: int) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            z_dim (int): Dimension of the noise vector used for image generation.\n",
    "            img_channels (int): Number of channels in the generated image.\n",
    "            hidden_units (int): Number of neurons in the hidden layer.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # First generator block\n",
    "            self._generator_block(\n",
    "                in_channels=z_dim,\n",
    "                out_channels=hidden_units * 16,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            # Rest of the generator blocks...\n",
    "            self._generator_block(\n",
    "                in_channels=hidden_units * 16, out_channels=hidden_units * 8\n",
    "            ),\n",
    "            self._generator_block(\n",
    "                in_channels=hidden_units * 8, out_channels=hidden_units * 4\n",
    "            ),\n",
    "            self._generator_block(\n",
    "                in_channels=hidden_units * 4, out_channels=hidden_units * 2\n",
    "            ),\n",
    "            # Final block\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=hidden_units * 2,\n",
    "                out_channels=img_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.Tanh(),  # Output range: [-1, 1]\n",
    "        )\n",
    "\n",
    "    def _generator_block(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 4,\n",
    "        stride: int = 2,\n",
    "        padding: int = 1,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Creates a single generator block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int, optional): Size of convolution filter. Defaults to 4.\n",
    "            stride (int, optional): Stride size. Defaults to 2.\n",
    "            padding (int, optional): Amount of pixels added around the image. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Returns a tensor with performed convolutions.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.gen(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"Initializes weights using a normal distribution with mean 0 and std 0.02.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The generator or discriminator model.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        # if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def test_weights_initialization() -> None:\n",
    "    \"\"\"Tests the initialize_weights() function\"\"\"\n",
    "    N, C, H, W = 8, 3, 64, 64\n",
    "    z_dim = 100\n",
    "\n",
    "    x = torch.randn(\n",
    "        (N, C, H, W)\n",
    "    )  # Simulate a random batch of images of shape N x C x H x W\n",
    "\n",
    "    ### Test Discriminator\n",
    "    disc = Discriminator(img_channels=C, hidden_units=8)\n",
    "    initialize_weights(model=disc)\n",
    "\n",
    "    # There should be outputet one prediction per image\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminators weights are not initialized correctly.\"\n",
    "\n",
    "    ### Test Generator\n",
    "    gen = Generator(z_dim=z_dim, img_channels=C, hidden_units=8)\n",
    "    initialize_weights(model=gen)\n",
    "    \n",
    "    noise = torch.randn((N, z_dim, 1, 1))\n",
    "    fake_image = gen(noise)\n",
    "    \n",
    "    assert fake_image.shape == (\n",
    "        N,\n",
    "        C,\n",
    "        H,\n",
    "        W,\n",
    "    ), f\"Generators weights are not initialized correctly. Instead of ({N}, {C}, {H}, {W}) they are {fake_image.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch [0/64] | Batch 0/1875 | Loss Disc: 0.6837, loss Gen: 0.7798\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters for training\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 2e-4\n",
    "batch_size = 32\n",
    "img_size = 64\n",
    "img_channels = 1\n",
    "z_dim = 100\n",
    "num_epochs = 64\n",
    "hidden_units = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setup dataset and datalaoder\n",
    "dataset = MNIST(root='/data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Setup Generator\n",
    "gen = Generator(z_dim=z_dim, img_channels=img_channels, hidden_units=hidden_units).to(device)\n",
    "initialize_weights(gen)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup Discriminator\n",
    "disc = Discriminator(img_channels=img_channels, hidden_units=hidden_units).to(device)\n",
    "initialize_weights(disc)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# In order to see progress during training for educational purposes\n",
    "# ( Random noise is generally preferred\n",
    "#  to promote diversity and robustness in the generated samples).\n",
    "fixed_noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Setup tensorboard (for experiment tracking)\n",
    "step = 0\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')\n",
    "\n",
    "# Set models to train mode\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "# Train loop\n",
    "mean_gen_loss, mean_disc_loss = 0, 0\n",
    "\n",
    "\n",
    "def log_images(real, fake, step, noise: torch.Tensor=fixed_noise) -> None:\n",
    "    with torch.no_grad():\n",
    "        fake = gen(noise)\n",
    "\n",
    "        grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "        writer_real.add_image(\"Real images\", grid_real, global_step=step)\n",
    "\n",
    "        grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "        writer_fake.add_image(\"Fake (Generated) images\", grid_fake, global_step=step)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        # Access a batch of real images and move them on the GPU if possible\n",
    "        real = real.to(device)\n",
    "\n",
    "        # Generate batch of random noise and move it on the GPU if possible\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train the Discriminator ###\n",
    "        disc_real_predicitons = disc(real)#.reshape(-1)# .value(-1)\n",
    "        disc_fake_predictions = disc(fake)#.reshape(-1)#.value(-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        disc_real_loss = criterion(disc_real_predicitons, torch.ones_like(disc_real_predicitons))\n",
    "        disc_fake_loss = criterion(disc_fake_predictions, torch.zeros_like(disc_fake_predictions))\n",
    "        disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
    "\n",
    "        # Keep track of the mean discriminator loss\n",
    "        mean_disc_loss = disc_loss.item() / batch_size\n",
    "\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train the Generator ###\n",
    "        disc_fake_pred = disc(fake)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen.zero_grad()\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        mean_gen_loss += gen_loss.item() / batch_size\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] | Batch {batch_idx}/{len(dataloader)} | Loss Disc: {disc_loss:.4f}, loss Gen: {gen_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Log results to Tensorboard\n",
    "        log_images(real=real, fake=fake, noise=fixed_noise, step=step)\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
