{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first DCGAN ðŸŒ±\n",
    "Deep Convolutional Generative Adversarial Network architecture implementation.\n",
    "- DCGAN [paper](https://arxiv.org/abs/1511.06434).\n",
    "- [Papers with Code](https://paperswithcode.com/method/dcgan) explanation.\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Gan Archtecture Scheme</b>\n",
    "</font>\n",
    "</summary>\n",
    "<div>\n",
    "<img src = \"layers.png\" width=800>\n",
    "</div>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, hidden_units) -> None:\n",
    "        \"\"\"Discriminator model for DCGAN architecture based\n",
    "         on Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks paper.\n",
    "         Paper: https://arxiv.org/abs/1511.06434\n",
    "\n",
    "        Args:\n",
    "            img_channels (int): Number of channels of the image (for example for RGB its 3).\n",
    "            hidden_units (int): Number of neurons in hidden layer.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # First discriminator block (bo batch norm)\n",
    "            nn.Conv2d(img_channels, hidden_units, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Rest of the blocks...\n",
    "            self._block(in_channels=hidden_units, out_channels=hidden_units * 2),\n",
    "            self._block(in_channels=hidden_units * 2, out_channels=hidden_units * 4),\n",
    "            self._block(in_channels=hidden_units * 4, out_channels=hidden_units * 8),\n",
    "            nn.Conv2d(in_channels=hidden_units * 8, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(), # Output range [0, 1]\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel__size=4, stride=2, padding=1) -> torch.Tensor:\n",
    "        \"\"\"Creates a single discriminator block consisting of a convolution layer, batch normalization and Leaky ReLU.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "                out_channels (int): Number of output channels.\n",
    "                kernel_size (int, optional): Size of convolution filter. Defaults to 4.\n",
    "                stride (int, optional): Stride size. Defaults to 2.\n",
    "                padding (int, optional): Amount of pixels added around the image. Defaults to 1.\n",
    "        Returns:\n",
    "            torch.Tensor: Returns a tensor with performed convolutions.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel__size, stride, padding, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim: int, img_channels: int, hidden_units: int) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            z_dim (int): Dimension of the noise vector used for image generation.\n",
    "            img_channels (int): Number of channels in the generated image.\n",
    "            hidden_units (int): Number of neurons in the hidden layer.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # First generator block\n",
    "            self._generator_block(\n",
    "                in_channels=z_dim,\n",
    "                out_channels=hidden_units * 16,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            # Rest of the generator blocks...\n",
    "            self._generator_block(\n",
    "                in_channels=hidden_units * 16, out_channels=hidden_units * 8\n",
    "            ),\n",
    "            self._generator_block(\n",
    "                in_channels=hidden_units * 8, out_channels=hidden_units * 4\n",
    "            ),\n",
    "            self._generator_block(\n",
    "                in_channels=hidden_units * 4, out_channels=hidden_units * 2\n",
    "            ),\n",
    "            # Final block\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=hidden_units * 2,\n",
    "                out_channels=img_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.Tanh(),  # Output range: [-1, 1]\n",
    "        )\n",
    "\n",
    "    def _generator_block(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 4,\n",
    "        stride: int = 2,\n",
    "        padding: int = 1,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Creates a single generator block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int, optional): Size of convolution filter. Defaults to 4.\n",
    "            stride (int, optional): Stride size. Defaults to 2.\n",
    "            padding (int, optional): Amount of pixels added around the image. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Returns a tensor with performed convolutions.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.gen(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"Initializes weights using a normal distribution with mean 0 and std 0.02.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The generator or discriminator model.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        # if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def test_weights_initialization() -> None:\n",
    "    \"\"\"Tests the initialize_weights() function\"\"\"\n",
    "    N, C, H, W = 8, 3, 64, 64\n",
    "    z_dim = 100\n",
    "\n",
    "    x = torch.randn(\n",
    "        (N, C, H, W)\n",
    "    )  # Simulate a random batch of images of shape N x C x H x W\n",
    "\n",
    "    ### Test Discriminator\n",
    "    disc = Discriminator(img_channels=C, hidden_units=8)\n",
    "    initialize_weights(model=disc)\n",
    "\n",
    "    # There should be outputet one prediction per image\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminators weights are not initialized correctly.\"\n",
    "\n",
    "    ### Test Generator\n",
    "    gen = Generator(z_dim=z_dim, img_channels=C, hidden_units=8)\n",
    "    initialize_weights(model=gen)\n",
    "    \n",
    "    noise = torch.randn((N, z_dim, 1, 1))\n",
    "    fake_image = gen(noise)\n",
    "    \n",
    "    assert fake_image.shape == (\n",
    "        N,\n",
    "        C,\n",
    "        H,\n",
    "        W,\n",
    "    ), f\"Generators weights are not initialized correctly. Instead of ({N}, {C}, {H}, {W}) they are {fake_image.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN will be trained on cuda.\n",
      "Epoch [0/64] | Batch 0/469 | Loss Disc: 0.6950, loss Gen: 0.7717\n",
      "Epoch [0/64] | Batch 50/469 | Loss Disc: 0.0490, loss Gen: 2.9549\n",
      "Epoch [0/64] | Batch 100/469 | Loss Disc: 0.0135, loss Gen: 4.1634\n",
      "Epoch [0/64] | Batch 150/469 | Loss Disc: 0.0061, loss Gen: 4.9695\n",
      "Epoch [0/64] | Batch 200/469 | Loss Disc: 0.0033, loss Gen: 5.5750\n",
      "Epoch [0/64] | Batch 250/469 | Loss Disc: 0.0021, loss Gen: 6.0347\n",
      "Epoch [0/64] | Batch 300/469 | Loss Disc: 0.0015, loss Gen: 6.4137\n",
      "Epoch [0/64] | Batch 350/469 | Loss Disc: 0.0011, loss Gen: 6.7260\n",
      "Epoch [0/64] | Batch 400/469 | Loss Disc: 0.0008, loss Gen: 6.9962\n",
      "Epoch [0/64] | Batch 450/469 | Loss Disc: 0.0006, loss Gen: 7.2354\n",
      "Epoch [1/64] | Batch 0/469 | Loss Disc: 0.0006, loss Gen: 7.3153\n",
      "Epoch [1/64] | Batch 50/469 | Loss Disc: 0.0005, loss Gen: 7.5221\n",
      "Epoch [1/64] | Batch 100/469 | Loss Disc: 0.0004, loss Gen: 7.7031\n",
      "Epoch [1/64] | Batch 150/469 | Loss Disc: 0.0003, loss Gen: 7.8669\n",
      "Epoch [1/64] | Batch 200/469 | Loss Disc: 0.0003, loss Gen: 8.0218\n",
      "Epoch [1/64] | Batch 250/469 | Loss Disc: 0.0003, loss Gen: 8.1707\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDCGAN will be trained on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (real, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     69\u001b[0m         \u001b[39m# Access a batch of real images and move them on the GPU if possible\u001b[39;00m\n\u001b[0;32m     70\u001b[0m         real \u001b[39m=\u001b[39m real\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m         \u001b[39m# Generate batch of random noise and move it on the GPU if possible\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[0;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\gans\\Lib\\site-packages\\PIL\\Image.py:2193\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2185\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[0;32m   2186\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[0;32m   2187\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   2188\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   2189\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   2190\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   2191\u001b[0m         )\n\u001b[1;32m-> 2193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Hyperparameters for training\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 2e-4\n",
    "batch_size = 128\n",
    "img_size = 64\n",
    "img_channels = 1\n",
    "z_dim = 100\n",
    "num_epochs = 64\n",
    "hidden_units = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setup dataset and datalaoder\n",
    "dataset = MNIST(root='/data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Setup Generator\n",
    "gen = Generator(z_dim=z_dim, img_channels=img_channels, hidden_units=hidden_units).to(device)\n",
    "initialize_weights(gen)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup Discriminator\n",
    "disc = Discriminator(img_channels=img_channels, hidden_units=hidden_units).to(device)\n",
    "initialize_weights(disc)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# In order to see progress during training for educational purposes\n",
    "# ( Random noise is generally preferred\n",
    "#  to promote diversity and robustness in the generated samples).\n",
    "fixed_noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Setup tensorboard (for experiment tracking)\n",
    "step = 0\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')\n",
    "\n",
    "# Set models to train mode\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "# Train loop\n",
    "mean_gen_loss, mean_disc_loss = 0, 0\n",
    "\n",
    "\n",
    "def log_images(real, fake, step, noise: torch.Tensor=fixed_noise) -> None:\n",
    "    with torch.no_grad():\n",
    "        fake = gen(noise)\n",
    "\n",
    "        grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "        writer_real.add_image(\"Real images\", grid_real, global_step=step)\n",
    "\n",
    "        grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "        writer_fake.add_image(\"Fake (Generated) images\", grid_fake, global_step=step)\n",
    "\n",
    "\n",
    "print(f\"DCGAN will be trained on {device}.\")\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        # Access a batch of real images and move them on the GPU if possible\n",
    "        real = real.to(device)\n",
    "\n",
    "        # Generate batch of random noise and move it on the GPU if possible\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train the Discriminator ###\n",
    "        disc_real_predicitons = disc(real)#.reshape(-1)# .value(-1)\n",
    "        disc_fake_predictions = disc(fake)#.reshape(-1)#.value(-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        disc_real_loss = criterion(disc_real_predicitons, torch.ones_like(disc_real_predicitons))\n",
    "        disc_fake_loss = criterion(disc_fake_predictions, torch.zeros_like(disc_fake_predictions))\n",
    "        disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
    "\n",
    "        # Keep track of the mean discriminator loss\n",
    "        mean_disc_loss = disc_loss.item() / batch_size\n",
    "\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train the Generator ###\n",
    "        disc_fake_pred = disc(fake)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen.zero_grad()\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        mean_gen_loss += gen_loss.item() / batch_size\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] | Batch {batch_idx}/{len(dataloader)} | Loss Disc: {disc_loss:.4f}, loss Gen: {gen_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Log results to Tensorboard\n",
    "        log_images(real=real, fake=fake, noise=fixed_noise, step=step)\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
